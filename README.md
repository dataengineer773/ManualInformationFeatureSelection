Mutual information from the feild of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection, Mutual information is strightforward
 when considering  the distribution of two discrete(categorical or ordinal) variables , such as categorical input and categorical output data, Nverthelse it can be adapted for use with numerical input and output 
 data and in the final we use barchart of the feature importance scores for each input feature is created , Compared to the correlation feature slection method we can clearly see many more feature scored as being
 relevant.
